{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.kuhn.kuhn import KuhnPoker\n",
    "from agents.counterfactualregret import CounterFactualRegret\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = KuhnPoker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_classes = [ CounterFactualRegret, CounterFactualRegret ]\n",
    "my_agents = {}\n",
    "g.reset()\n",
    "for i, agent in enumerate(g.agents):\n",
    "    my_agents[agent] = agent_classes[i](game=g, agent=agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent agent_0\n",
      "Agent agent_0 policies:\n",
      "OrderedDict([('0', array([0.66350825, 0.33649175])), ('0b', array([9.99970094e-01, 2.99060949e-05])), ('0p', array([0.66332467, 0.33667533])), ('0pb', array([9.99966260e-01, 3.37401801e-05])), ('1', array([9.99790526e-01, 2.09474403e-04])), ('1b', array([0.65767273, 0.34232727])), ('1p', array([9.99919855e-01, 8.01450626e-05])), ('1pb', array([0.32780034, 0.67219966])), ('2', array([7.05586736e-05, 9.99929441e-01])), ('2b', array([3.00372462e-05, 9.99969963e-01])), ('2p', array([9.01117386e-05, 9.99909888e-01])), ('2pb', array([0.26614015, 0.73385985]))])\n",
      "\n",
      "Training agent agent_1\n",
      "Agent agent_1 policies:\n",
      "OrderedDict([('0', array([0.67969854, 0.32030146])), ('0b', array([9.99969809e-01, 3.01905021e-05])), ('0p', array([0.66713943, 0.33286057])), ('0pb', array([9.99966954e-01, 3.30460033e-05])), ('1', array([9.99659972e-01, 3.40028355e-04])), ('1b', array([0.65056877, 0.34943123])), ('1p', array([9.99955113e-01, 4.48873328e-05])), ('1pb', array([0.33200873, 0.66799127])), ('2', array([0.00766375, 0.99233625])), ('2b', array([2.98837522e-05, 9.99970116e-01])), ('2p', array([2.98837522e-05, 9.99970116e-01])), ('2pb', array([0.0029703, 0.9970297]))])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for agent in g.agents:\n",
    "    print('Training agent ' + agent)\n",
    "    my_agents[agent].train(100000)\n",
    "    print('Agent ' + agent + ' policies:')\n",
    "    print(OrderedDict(map(lambda n: (n, my_agents[agent].node_dict[n].policy()), sorted(my_agents[agent].node_dict.keys()))))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards: {'agent_0': 0.05, 'agent_1': -0.05}\n"
     ]
    }
   ],
   "source": [
    "cum_rewards = dict(map(lambda agent: (agent, 0.), g.agents))\n",
    "niter = 2000\n",
    "for _ in range(niter):\n",
    "    g.reset()\n",
    "    turn = 0\n",
    "    while not g.done():\n",
    "        #print('Turn: ', turn)\n",
    "        #print('\\tPlayer: ', g.agent_selection)\n",
    "        #print('\\tObservation: ', g.observe(g.agent_selection))\n",
    "        a = my_agents[g.agent_selection].action()\n",
    "        #print('\\tAction: ', g._moves[a])\n",
    "        g.step(action=a)\n",
    "        turn += 1\n",
    "    #print('Rewards: ', g.rewards)\n",
    "    for agent in g.agents:\n",
    "        cum_rewards[agent] += g.rewards[agent]\n",
    "print('Average rewards:', dict(map(lambda agent: (agent, cum_rewards[agent]/niter), g.agents)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check learned policies against theoretical policies:\n"
     ]
    }
   ],
   "source": [
    "print('Check learned policies against theoretical policies:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: J_ - History: [] - Probability of betting: 0.33649174933368675\n"
     ]
    }
   ],
   "source": [
    "JX_b = my_agents[g.agents[0]].node_dict['0'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: J_ - History: [] - Probability of betting: {JX_b}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: Q_ - History: pb - Probability of betting: 0.6721996620132858 - Theoretic value: 0.66982508266702 -  Difference: 0.002374579346265837\n"
     ]
    }
   ],
   "source": [
    "QX_pb_b = my_agents[g.agents[0]].node_dict['1pb'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: Q_ - History: pb - Probability of betting: {QX_pb_b} - Theoretic value: {JX_b+1/3} -  Difference: {abs(QX_pb_b - (JX_b+1/3))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: K_ - History: [] - Probability of betting: 0.9999294413263614 - Theoretic value: 1.0094752480010603 -  Difference: 0.009545806674698842\n"
     ]
    }
   ],
   "source": [
    "KX_b = my_agents[g.agents[0]].node_dict['2'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: K_ - History: [] - Probability of betting: {KX_b} - Theoretic value: {3 * JX_b} -  Difference: {abs(KX_b - 3 * JX_b)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: _J - History: p - Probability of betting: 0.33667533339142314 - Theoretic value: 0.3333333333333333 -  Difference: 0.0033420000580898224\n"
     ]
    }
   ],
   "source": [
    "XJ_p_b = my_agents[g.agents[0]].node_dict['0p'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: _J - History: p - Probability of betting: {XJ_p_b} - Theoretic value: {1/3} -  Difference: {abs(XJ_p_b - 1/3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: _Q - History: b - Probability of betting: 0.34232726763897287 - Theoretic value: 0.3333333333333333 -  Difference: 0.00899393430563955\n"
     ]
    }
   ],
   "source": [
    "XQ_b_b = my_agents[g.agents[0]].node_dict['1b'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: _Q - History: b - Probability of betting: {XQ_b_b} - Theoretic value: {1/3} -  Difference: {abs(XQ_b_b - 1/3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
