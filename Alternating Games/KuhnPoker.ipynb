{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.kuhn.kuhn import KuhnPoker\n",
    "from agents.counterfactualregret import CounterFactualRegret\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = KuhnPoker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.reset()\n",
    "g._hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_classes = [ CounterFactualRegret, CounterFactualRegret ]\n",
    "my_agents = {}\n",
    "g.reset()\n",
    "for i, agent in enumerate(g.agents):\n",
    "    my_agents[agent] = agent_classes[i](game=g, agent=agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent agent_0\n",
      "Agent agent_0 policies:\n",
      "OrderedDict([('0', array([9.99969948e-01, 3.00516889e-05])), ('0b', array([9.99970069e-01, 2.99311583e-05])), ('0p', array([9.99970069e-01, 2.99311583e-05])), ('0pb', array([9.99977461e-01, 2.25391053e-05])), ('1', array([9.99970140e-01, 2.98596596e-05])), ('1b', array([0.5, 0.5])), ('1p', array([9.99969861e-01, 3.01386377e-05])), ('1pb', array([0.5, 0.5])), ('2', array([9.99969913e-01, 3.00869513e-05])), ('2b', array([0.5, 0.5])), ('2p', array([9.99970072e-01, 2.99284710e-05])), ('2pb', array([2.25655529e-05, 9.99977434e-01]))])\n",
      "\n",
      "Training agent agent_1\n",
      "Agent agent_1 policies:\n",
      "OrderedDict([('0', array([9.99970351e-01, 2.96489564e-05])), ('0b', array([9.99969967e-01, 3.00327357e-05])), ('0p', array([9.99969967e-01, 3.00327357e-05])), ('0pb', array([9.99977763e-01, 2.22370469e-05])), ('1', array([9.99969936e-01, 3.00643377e-05])), ('1b', array([0.5, 0.5])), ('1p', array([9.99970045e-01, 2.99553665e-05])), ('1pb', array([0.5, 0.5])), ('2', array([9.99969709e-01, 3.02910974e-05])), ('2b', array([3.00093029e-05, 9.99969991e-01])), ('2p', array([9.99969991e-01, 3.00093029e-05])), ('2pb', array([2.27186672e-05, 9.99977281e-01]))])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for agent in g.agents:\n",
    "    print('Training agent ' + agent)\n",
    "    my_agents[agent].train(100000)\n",
    "    print('Agent ' + agent + ' policies:')\n",
    "    print(OrderedDict(map(lambda n: (n, my_agents[agent].node_dict[n].policy()), sorted(my_agents[agent].node_dict.keys()))))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards: {'agent_0': 0.017, 'agent_1': -0.017}\n"
     ]
    }
   ],
   "source": [
    "cum_rewards = dict(map(lambda agent: (agent, 0.), g.agents))\n",
    "niter = 2000\n",
    "for _ in range(niter):\n",
    "    g.reset()\n",
    "    turn = 0\n",
    "    while not g.done():\n",
    "        #print('Turn: ', turn)\n",
    "        #print('\\tPlayer: ', g.agent_selection)\n",
    "        #print('\\tObservation: ', g.observe(g.agent_selection))\n",
    "        a = my_agents[g.agent_selection].action()\n",
    "        #print('\\tAction: ', g._moves[a])\n",
    "        g.step(action=a)\n",
    "        turn += 1\n",
    "    #print('Rewards: ', g.rewards)\n",
    "    for agent in g.agents:\n",
    "        cum_rewards[agent] += g.rewards[agent]\n",
    "print('Average rewards:', dict(map(lambda agent: (agent, cum_rewards[agent]/niter), g.agents)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check learned policies against theoretical policies:\n"
     ]
    }
   ],
   "source": [
    "print('Check learned policies against theoretical policies:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: J_ - History: [] - Probability of betting: 0.11362305231804722\n"
     ]
    }
   ],
   "source": [
    "JX_b = my_agents[g.agents[0]].node_dict['0'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: J_ - History: [] - Probability of betting: {JX_b}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: Q_ - History: pb - Probability of betting: 0.43739620157029147 - Theoretic value: 0.4469563856513805 -  Difference: 0.009560184081089051\n"
     ]
    }
   ],
   "source": [
    "QX_pb_b = my_agents[g.agents[0]].node_dict['1pb'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: Q_ - History: pb - Probability of betting: {QX_pb_b} - Theoretic value: {JX_b+1/3} -  Difference: {abs(QX_pb_b - (JX_b+1/3))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: K_ - History: [] - Probability of betting: 0.33221948785943334 - Theoretic value: 0.34086915695414166 -  Difference: 0.008649669094708323\n"
     ]
    }
   ],
   "source": [
    "KX_b = my_agents[g.agents[0]].node_dict['2'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: K_ - History: [] - Probability of betting: {KX_b} - Theoretic value: {3 * JX_b} -  Difference: {abs(KX_b - 3 * JX_b)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: _J - History: p - Probability of betting: 0.3359600141817036 - Theoretic value: 0.3333333333333333 -  Difference: 0.002626680848370311\n"
     ]
    }
   ],
   "source": [
    "XJ_p_b = my_agents[g.agents[0]].node_dict['0p'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: _J - History: p - Probability of betting: {XJ_p_b} - Theoretic value: {1/3} -  Difference: {abs(XJ_p_b - 1/3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: _Q - History: b - Probability of betting: 0.5 - Theoretic value: 0.3333333333333333 -  Difference: 0.16666666666666669\n"
     ]
    }
   ],
   "source": [
    "XQ_b_b = my_agents[g.agents[0]].node_dict['1b'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: _Q - History: b - Probability of betting: {XQ_b_b} - Theoretic value: {1/3} -  Difference: {abs(XQ_b_b - 1/3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo_games",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
