{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.kuhn.kuhn import KuhnPoker\n",
    "from agents.counterfactualregret import CounterFactualRegret\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = KuhnPoker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.reset()\n",
    "g._hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_classes = [ CounterFactualRegret, CounterFactualRegret ]\n",
    "my_agents = {}\n",
    "g.reset()\n",
    "for i, agent in enumerate(g.agents):\n",
    "    my_agents[agent] = agent_classes[i](game=g, agent=agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent agent_0\n",
      "Agent agent_0 policies:\n",
      "OrderedDict([('0', array([0.70101354, 0.29898646])), ('0b', array([9.99969895e-01, 3.01050667e-05])), ('0p', array([0.67119976, 0.32880024])), ('0pb', array([9.99968014e-01, 3.19856692e-05])), ('1', array([9.99924047e-01, 7.59532128e-05])), ('1b', array([0.66011938, 0.33988062])), ('1p', array([9.99924875e-01, 7.51247070e-05])), ('1pb', array([0.35002294, 0.64997706])), ('2', array([0.07429184, 0.92570816])), ('2b', array([2.98436194e-05, 9.99970156e-01])), ('2p', array([5.96872389e-05, 9.99940313e-01])), ('2pb', array([3.00038707e-04, 9.99699961e-01]))])\n",
      "\n",
      "Training agent agent_1\n",
      "Agent agent_1 policies:\n",
      "OrderedDict([('0', array([0.8312116, 0.1687884])), ('0b', array([9.99969803e-01, 3.01968837e-05])), ('0p', array([0.66158083, 0.33841917])), ('0pb', array([9.99973036e-01, 2.69635448e-05])), ('1', array([9.99526718e-01, 4.73282002e-04])), ('1b', array([0.66364265, 0.33635735])), ('1p', array([9.99738454e-01, 2.61545641e-04])), ('1pb', array([0.47116219, 0.52883781])), ('2', array([0.41620435, 0.58379565])), ('2b', array([2.96991476e-05, 9.99970301e-01])), ('2p', array([1.18796591e-04, 9.99881203e-01])), ('2pb', array([5.42228662e-05, 9.99945777e-01]))])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for agent in g.agents:\n",
    "    print('Training agent ' + agent)\n",
    "    my_agents[agent].train(100000)\n",
    "    print('Agent ' + agent + ' policies:')\n",
    "    print(OrderedDict(map(lambda n: (n, my_agents[agent].node_dict[n].policy()), sorted(my_agents[agent].node_dict.keys()))))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards: {'agent_0': 0.017, 'agent_1': -0.017}\n"
     ]
    }
   ],
   "source": [
    "cum_rewards = dict(map(lambda agent: (agent, 0.), g.agents))\n",
    "niter = 2000\n",
    "for _ in range(niter):\n",
    "    g.reset()\n",
    "    turn = 0\n",
    "    while not g.done():\n",
    "        #print('Turn: ', turn)\n",
    "        #print('\\tPlayer: ', g.agent_selection)\n",
    "        #print('\\tObservation: ', g.observe(g.agent_selection))\n",
    "        a = my_agents[g.agent_selection].action()\n",
    "        #print('\\tAction: ', g._moves[a])\n",
    "        g.step(action=a)\n",
    "        turn += 1\n",
    "    #print('Rewards: ', g.rewards)\n",
    "    for agent in g.agents:\n",
    "        cum_rewards[agent] += g.rewards[agent]\n",
    "print('Average rewards:', dict(map(lambda agent: (agent, cum_rewards[agent]/niter), g.agents)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check learned policies against theoretical policies:\n"
     ]
    }
   ],
   "source": [
    "print('Check learned policies against theoretical policies:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: J_ - History: [] - Probability of betting: 0.11362305231804722\n"
     ]
    }
   ],
   "source": [
    "JX_b = my_agents[g.agents[0]].node_dict['0'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: J_ - History: [] - Probability of betting: {JX_b}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: Q_ - History: pb - Probability of betting: 0.43739620157029147 - Theoretic value: 0.4469563856513805 -  Difference: 0.009560184081089051\n"
     ]
    }
   ],
   "source": [
    "QX_pb_b = my_agents[g.agents[0]].node_dict['1pb'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: Q_ - History: pb - Probability of betting: {QX_pb_b} - Theoretic value: {JX_b+1/3} -  Difference: {abs(QX_pb_b - (JX_b+1/3))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: K_ - History: [] - Probability of betting: 0.33221948785943334 - Theoretic value: 0.34086915695414166 -  Difference: 0.008649669094708323\n"
     ]
    }
   ],
   "source": [
    "KX_b = my_agents[g.agents[0]].node_dict['2'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: K_ - History: [] - Probability of betting: {KX_b} - Theoretic value: {3 * JX_b} -  Difference: {abs(KX_b - 3 * JX_b)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: _J - History: p - Probability of betting: 0.3359600141817036 - Theoretic value: 0.3333333333333333 -  Difference: 0.002626680848370311\n"
     ]
    }
   ],
   "source": [
    "XJ_p_b = my_agents[g.agents[0]].node_dict['0p'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: _J - History: p - Probability of betting: {XJ_p_b} - Theoretic value: {1/3} -  Difference: {abs(XJ_p_b - 1/3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: _Q - History: b - Probability of betting: 0.5 - Theoretic value: 0.3333333333333333 -  Difference: 0.16666666666666669\n"
     ]
    }
   ],
   "source": [
    "XQ_b_b = my_agents[g.agents[0]].node_dict['1b'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: _Q - History: b - Probability of betting: {XQ_b_b} - Theoretic value: {1/3} -  Difference: {abs(XQ_b_b - 1/3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo_games",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
