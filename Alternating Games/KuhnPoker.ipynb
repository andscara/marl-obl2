{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.kuhn.kuhn import KuhnPoker\n",
    "from agents.counterfactualregret import CounterFactualRegret\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = KuhnPoker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_classes = [ CounterFactualRegret, CounterFactualRegret ]\n",
    "my_agents = {}\n",
    "g.reset()\n",
    "for i, agent in enumerate(g.agents):\n",
    "    my_agents[agent] = agent_classes[i](game=g, agent=agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent agent_0\n"
     ]
    }
   ],
   "source": [
    "for agent in g.agents:\n",
    "    print('Training agent ' + agent)\n",
    "    my_agents[agent].train(100000)\n",
    "    print('Agent ' + agent + ' policies:')\n",
    "    print(OrderedDict(map(lambda n: (n, my_agents[agent].node_dict[n].policy()), sorted(my_agents[agent].node_dict.keys()))))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def graficar_politica_tree(politicas: dict[str, np.ndarray], nombre='Agent_Policy'):\n",
    "    dot = Digraph(comment='CFR Policy Tree')\n",
    "    dot.attr('node', shape='box', style='rounded,filled', fillcolor='lightgrey')\n",
    "\n",
    "    for estado, probs in politicas.items():\n",
    "        etiqueta = f\"{estado}\\\\n[p: {probs[0]:.2f}, b: {probs[1]:.2f}]\"\n",
    "        dot.node(estado, etiqueta)\n",
    "\n",
    "        if len(estado) > 1:\n",
    "            padre = estado[:-1]\n",
    "            accion = estado[-1]\n",
    "            dot.edge(padre, estado, label=accion)\n",
    "\n",
    "    dot.render(filename=nombre, format='png', cleanup=True)\n",
    "    print(f\"Diagrama guardado como {nombre}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicas = OrderedDict(map(lambda n: (n, my_agents[agent].node_dict[n].policy()), sorted(my_agents[agent].node_dict.keys())))\n",
    "graficar_politica_tree(politicas, nombre=\"agent_0_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards: {'agent_0': -0.098, 'agent_1': 0.098}\n"
     ]
    }
   ],
   "source": [
    "cum_rewards = dict(map(lambda agent: (agent, 0.), g.agents))\n",
    "niter = 2000\n",
    "for _ in range(niter):\n",
    "    g.reset()\n",
    "    turn = 0\n",
    "    while not g.done():\n",
    "        #print('Turn: ', turn)\n",
    "        #print('\\tPlayer: ', g.agent_selection)\n",
    "        #print('\\tObservation: ', g.observe(g.agent_selection))\n",
    "        a = my_agents[g.agent_selection].action()\n",
    "        #print('\\tAction: ', g._moves[a])\n",
    "        g.step(action=a)\n",
    "        turn += 1\n",
    "    #print('Rewards: ', g.rewards)\n",
    "    for agent in g.agents:\n",
    "        cum_rewards[agent] += g.rewards[agent]\n",
    "print('Average rewards:', dict(map(lambda agent: (agent, cum_rewards[agent]/niter), g.agents)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check learned policies against theoretical policies:\n"
     ]
    }
   ],
   "source": [
    "print('Check learned policies against theoretical policies:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: J_ - History: [] - Probability of betting: 0.10355011505845793\n"
     ]
    }
   ],
   "source": [
    "JX_b = my_agents[g.agents[0]].node_dict['0'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: J_ - History: [] - Probability of betting: {JX_b}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: Q_ - History: pb - Probability of betting: 0.43758577337494275 - Theoretic value: 0.4368834483917913 -  Difference: 0.0007023249831514811\n"
     ]
    }
   ],
   "source": [
    "QX_pb_b = my_agents[g.agents[0]].node_dict['1pb'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: Q_ - History: pb - Probability of betting: {QX_pb_b} - Theoretic value: {JX_b+1/3} -  Difference: {abs(QX_pb_b - (JX_b+1/3))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: K_ - History: [] - Probability of betting: 0.32074509924715594 - Theoretic value: 0.3106503451753738 -  Difference: 0.01009475407178212\n"
     ]
    }
   ],
   "source": [
    "KX_b = my_agents[g.agents[0]].node_dict['2'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: K_ - History: [] - Probability of betting: {KX_b} - Theoretic value: {3 * JX_b} -  Difference: {abs(KX_b - 3 * JX_b)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: _J - History: p - Probability of betting: 0.33362013601498486 - Theoretic value: 0.3333333333333333 -  Difference: 0.00028680268165154343\n"
     ]
    }
   ],
   "source": [
    "XJ_p_b = my_agents[g.agents[0]].node_dict['0p'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: _J - History: p - Probability of betting: {XJ_p_b} - Theoretic value: {1/3} -  Difference: {abs(XJ_p_b - 1/3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: _Q - History: b - Probability of betting: 0.3274301069845622 - Theoretic value: 0.3333333333333333 -  Difference: 0.005903226348771107\n"
     ]
    }
   ],
   "source": [
    "XQ_b_b = my_agents[g.agents[0]].node_dict['1b'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: _Q - History: b - Probability of betting: {XQ_b_b} - Theoretic value: {1/3} -  Difference: {abs(XQ_b_b - 1/3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo_games",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
