{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.kuhn.kuhn3 import KuhnPoker3\n",
    "from agents.counterfactualregret import CounterFactualRegret\n",
    "from agents.agent_random import RandomAgent\n",
    "from agents.mcts import MonteCarloTreeSearch\n",
    "from agents.minimax import MiniMax\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kuhn Poker de 3 jugadores\n",
    "El Kuhn póker es una versión simplificada del poker. Es un juego de apuestas con información incompleta, es decir, no conocemos las cartas de los contrincantes. Las reglas báscicas son:  \n",
    "\n",
    "* **Mazo y reparto**  \n",
    "    Se utiliza un mazo reducido, en el caso de tres jugadores se utilizan cuatro cartas distintas ('J', 'Q', 'K', 'A'). Se baraja y reparte una carta a cada jugador y se deja una abajo. Los jugadores solo conocen su carta.\n",
    "* **Apuestas**  \n",
    "    * **Apuesta inicial:** Antes de ver las cartas cada jugador hace una pequeña apuesta inicial al pozo.  \n",
    "    * **Ronda de apuestas:** Luego de la apuesta inicial se juega una única ronda de apuestas en sentido horario.\n",
    "        * El primer jugador puede pasar (\"check\") o apostar.  \n",
    "        * Los próximos jugadores pueden irse (\"fold\") o igualar (\"call\").\n",
    "* **Desenlace:** \n",
    "    Al terminar la ronda de apuestas si todos los jugadores se retiraron el jugador que es \"mano\" gana la partida. Si al final de la ronda hay más de un jugador se comparan las cartas y el jugador con la carta más alta gana. El ganador se lleva el bote.  \n",
    "\n",
    "En definitiva, es un juego alternado, competitivo y de información incompleta ya que los agentes solo podrán observar su propia carta y las acciones de los demás. Esta información parcial es lo que se conoce como *Information set*.  \n",
    "Su versión de dos jugadores es muy conocida y esta versión de tres jugadores también ha sido bastante explorada y en ***AParameterized Family of Equilibrium Profiles for Three-Player Kuhn Poker*** hallan analiticamente el equilibrio de Nash del juego.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information sets terminales**  \n",
    "Los information sets terminales son:   \n",
    "**Profundidad 3**  \n",
    "'ppp', Gana jugador 1, recompensa: 0 \n",
    "'bpp', Gana jugador 1, recomepnsa: 0 \n",
    "'bbp', Compiten jugadores 1 y 2, recompensa: 1   \n",
    "'bpb', Compiten jugadores 1 y 3, recompensa: 2  \n",
    "'bbb', Compiten jugadores 1, 2 y 3, recomepnsa: 3    \n",
    "**Profundiad 4**  \n",
    "'pbpp', Gana jugador 2   \n",
    "'pbpb',   \n",
    "'pbbp',   \n",
    "'pbbb',  \n",
    "**Profundiad 5**  \n",
    "'ppbpp',   \n",
    "'ppbbp',   \n",
    "'ppbpb',   \n",
    "'ppbbb'  \n",
    "\n",
    "**Nota:** Los comentarios son asumiendo que el jugador que es mano (comienza jugando) es el jugador 1. Si la mano cambia, también los jugadores anotados.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = KuhnPoker3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# MiniMax seed=None, depth: int=sys.maxsize\n",
    "MonteCarloTreeSearch simulations: int=100, rollouts: int=10, selection: Callable[[MCTSNode], MCTSNode]=uct, max_depth=None, eval_name: str=\"eval\"\n",
    "# RandomAgent seed=None\n",
    "# CounterFactualRegret \n",
    "\"\"\"\n",
    "\n",
    "classes_parameters = {\n",
    "    'agent_0': [MiniMax,      {'seed': 42, 'depth': 3}],\n",
    "    'agent_1': [CounterFactualRegret,      {}],\n",
    "    'agent_2': [RandomAgent, {}]\n",
    "}\n",
    "\n",
    "my_agents = {}\n",
    "g.reset()\n",
    "for agent_id in g.agents:\n",
    "    AgentClass, params = classes_parameters[agent_id]\n",
    "    my_agents[agent_id] = AgentClass(game=g, agent=agent_id, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent agent_0 doesn't need training.\n",
      "Training agent agent_1\n",
      "Agent agent_1 policies:\n",
      "OrderedDict([('0', array([0.99865772, 0.00134228])), ('0b', array([0.00938338, 0.99061662])), ('0bb', array([0.5, 0.5])), ('0bp', array([0.99867021, 0.00132979])), ('0p', array([0.99798928, 0.00201072])), ('0pb', array([0.00398936, 0.99601064])), ('0pbb', array([0.5, 0.5])), ('0pbp', array([0.99899261, 0.00100739])), ('0pp', array([0.99867021, 0.00132979])), ('0ppb', array([0.4406638, 0.5593362])), ('0ppbb', array([0.5, 0.5])), ('0ppbp', array([0.99865772, 0.00134228])), ('1', array([0.94392833, 0.05607167])), ('1b', array([0.99744246, 0.00255754])), ('1bb', array([0.5, 0.5])), ('1bp', array([0.00620347, 0.99379653])), ('1p', array([0.92497006, 0.07502994])), ('1pb', array([0.97181186, 0.02818814])), ('1pbb', array([0.5, 0.5])), ('1pbp', array([0.77911373, 0.22088627])), ('1pp', array([0.95587052, 0.04412948])), ('1ppb', array([0.9947657, 0.0052343])), ('1ppbb', array([0.5, 0.5])), ('1ppbp', array([0.40770323, 0.59229677])), ('2', array([0.98877741, 0.01122259])), ('2b', array([0.01342036, 0.98657964])), ('2bb', array([0.99207784, 0.00792216])), ('2bp', array([0.84856416, 0.15143584])), ('2p', array([0.98998362, 0.01001638])), ('2pb', array([0.00636492, 0.99363508])), ('2pbb', array([0.99522885, 0.00477115])), ('2pbp', array([0.98519398, 0.01480602])), ('2pp', array([0.9449311, 0.0550689])), ('2ppb', array([0.00255739, 0.99744261])), ('2ppbb', array([0.99775585, 0.00224415])), ('2ppbp', array([0.77214463, 0.22785537])), ('3', array([0.99863388, 0.00136612])), ('3b', array([0.26012081, 0.73987919])), ('3bb', array([0.00135318, 0.99864682])), ('3bp', array([0.00135318, 0.99864682])), ('3p', array([0.98714082, 0.01285918])), ('3pb', array([0.01615568, 0.98384432])), ('3pbb', array([0.00102529, 0.99897471])), ('3pbp', array([0.00102529, 0.99897471])), ('3pp', array([0.94457413, 0.05542587])), ('3ppb', array([0.0056283, 0.9943717])), ('3ppbb', array([0.00259582, 0.99740418])), ('3ppbp', array([0.00194686, 0.99805314]))])\n",
      "\n",
      "Agent agent_1 doesn't need training.\n",
      "Agent agent_2 doesn't need training.\n"
     ]
    }
   ],
   "source": [
    "counter_facutal_agents_id = []\n",
    "for agent in g.agents:\n",
    "    if isinstance(my_agents[agent], CounterFactualRegret):\n",
    "        print('Training agent ' + agent)\n",
    "        counter_facutal_agents_id.append(agent)\n",
    "        my_agents[agent].train(3000)\n",
    "        print('Agent ' + agent + ' policies:')\n",
    "        print(OrderedDict(map(lambda n: (n, my_agents[agent].node_dict[n].policy()), sorted(my_agents[agent].node_dict.keys()))))\n",
    "        print('')\n",
    "    print('Agent ' + agent + \" doesn't need training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards: {'agent_0': -0.3455, 'agent_1': 0.76, 'agent_2': -0.4145}\n"
     ]
    }
   ],
   "source": [
    "cum_rewards = dict(map(lambda agent: (agent, 0.), g.agents))\n",
    "niter = 2000\n",
    "for _ in range(niter):\n",
    "    g.reset()\n",
    "    turn = 0\n",
    "    while not g.done():\n",
    "        #print('Turn: ', turn)\n",
    "        #print('\\tPlayer: ', g.agent_selection)\n",
    "        #print('\\tObservation: ', g.observe(g.agent_selection))\n",
    "        a = my_agents[g.agent_selection].action()\n",
    "        #print('\\tAction: ', g._moves[a])\n",
    "        g.step(action=a)\n",
    "        turn += 1\n",
    "    #print('Rewards: ', g.rewards)\n",
    "    for agent in g.agents:\n",
    "        cum_rewards[agent] += g.rewards[agent]\n",
    "print('Average rewards:', dict(map(lambda agent: (agent, cum_rewards[agent]/niter), g.agents)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check learned policies against theoretical policies:\n"
     ]
    }
   ],
   "source": [
    "print('Check learned policies against theoretical policies:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: J_ - History: [] - Probability of betting: 0.0013422818791946308\n",
      "Agent: 0 - Hand: Q_ - History: pb - Probability of betting: 0.028188136658131715 - Theoretic value: 0.3346756152125279 -  Difference: 0.30648747855439623\n",
      "Agent: 0 - Hand: K_ - History: [] - Probability of betting: 0.011222587000782717 - Theoretic value: 0.004026845637583892 -  Difference: 0.007195741363198825\n",
      "Agent: 0 - Hand: _J - History: p - Probability of betting: 0.0020107238605898124 - Theoretic value: 0.3333333333333333 -  Difference: 0.3313226094727435\n",
      "Agent: 0 - Hand: _J - History: p - Probability of betting: 0.0020107238605898124 - Theoretic value: 0.3333333333333333 -  Difference: 0.3313226094727435\n",
      "Agent: 0 - Hand: _Q - History: b - Probability of betting: 0.0025575447570332483 - Theoretic value: 0.3333333333333333 -  Difference: 0.3307757885763001\n"
     ]
    }
   ],
   "source": [
    "if len(counter_facutal_agents_id) > 0:\n",
    "    JX_b = my_agents[counter_facutal_agents_id[0]].node_dict['0'].policy()[1]\n",
    "    print(f'Agent: 0 - Hand: J_ - History: [] - Probability of betting: {JX_b}')\n",
    "\n",
    "    QX_pb_b = my_agents[counter_facutal_agents_id[0]].node_dict['1pb'].policy()[1]\n",
    "    print(f'Agent: 0 - Hand: Q_ - History: pb - Probability of betting: {QX_pb_b} - Theoretic value: {JX_b+1/3} -  Difference: {abs(QX_pb_b - (JX_b+1/3))}')\n",
    "\n",
    "    KX_b = my_agents[counter_facutal_agents_id[0]].node_dict['2'].policy()[1]\n",
    "    print(f'Agent: 0 - Hand: K_ - History: [] - Probability of betting: {KX_b} - Theoretic value: {3 * JX_b} -  Difference: {abs(KX_b - 3 * JX_b)}')\n",
    "\n",
    "    XJ_p_b = my_agents[counter_facutal_agents_id[0]].node_dict['0p'].policy()[1]\n",
    "    print(f'Agent: 0 - Hand: _J - History: p - Probability of betting: {XJ_p_b} - Theoretic value: {1/3} -  Difference: {abs(XJ_p_b - 1/3)}')\n",
    "\n",
    "    XJ_p_b = my_agents[counter_facutal_agents_id[0]].node_dict['0p'].policy()[1]\n",
    "    print(f'Agent: 0 - Hand: _J - History: p - Probability of betting: {XJ_p_b} - Theoretic value: {1/3} -  Difference: {abs(XJ_p_b - 1/3)}')\n",
    "\n",
    "    XQ_b_b = my_agents[counter_facutal_agents_id[0]].node_dict['1b'].policy()[1]\n",
    "    print(f'Agent: 0 - Hand: _Q - History: b - Probability of betting: {XQ_b_b} - Theoretic value: {1/3} -  Difference: {abs(XQ_b_b - 1/3)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo_games",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
